"""
Neural Architecture Search (NAS) System with Streamlit Interface
Complete implementation with Ngrok tunnel for Colab
All-in-one file: Install, train, and visualize through web interface
"""

# ============================================================================
# INSTALLATION SECTION
# ============================================================================

!pip install -q torch torchvision optuna mlflow streamlit pyngrok plotly kaleido

# ============================================================================
# CREATE STREAMLIT APP FILE
# ============================================================================

streamlit_app = '''
import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
import torchvision
import torchvision.transforms as transforms
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances
import mlflow
import mlflow.pytorch
from typing import List, Tuple, Dict
import numpy as np
import json
from datetime import datetime
import plotly.graph_objects as go
import time
import threading
import os

# Page config
st.set_page_config(
    page_title="Neural Architecture Search",
    page_icon="üß†",
    layout="wide"
)

# ============================================================================
# GLOBAL STATE
# ============================================================================

if "search_running" not in st.session_state:
    st.session_state.search_running = False
if "study" not in st.session_state:
    st.session_state.study = None
if "best_model" not in st.session_state:
    st.session_state.best_model = None
if "search_complete" not in st.session_state:
    st.session_state.search_complete = False
if "current_trial" not in st.session_state:
    st.session_state.current_trial = 0
if "total_trials" not in st.session_state:
    st.session_state.total_trials = 0
if "trial_results" not in st.session_state:
    st.session_state.trial_results = []

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================================================
# NEURAL NETWORK COMPONENTS
# ============================================================================

class NASNetwork(nn.Module):
    """Dynamically constructed neural network"""
    
    def __init__(self, config: dict, input_shape: Tuple[int, int, int], num_classes: int):
        super(NASNetwork, self).__init__()
        self.config = config
        
        layers = []
        in_channels = input_shape[0]
        h, w = input_shape[1], input_shape[2]
        
        # Convolutional layers
        for i in range(config["num_conv_layers"]):
            out_channels = config[f"conv_{i}_filters"]
            kernel_size = config[f"conv_{i}_kernel"]
            
            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(self._get_activation(config["activation"]))
            
            if config[f"conv_{i}_pooling"]:
                layers.append(nn.MaxPool2d(2))
                h, w = h // 2, w // 2
            
            if config["dropout"] > 0:
                layers.append(nn.Dropout2d(config["dropout"]))
            
            in_channels = out_channels
        
        self.conv_layers = nn.Sequential(*layers)
        flatten_size = in_channels * h * w
        
        # Fully connected layers
        fc_layers = []
        fc_input = flatten_size
        
        for i in range(config["num_fc_layers"]):
            fc_output = config[f"fc_{i}_units"]
            fc_layers.append(nn.Linear(fc_input, fc_output))
            fc_layers.append(self._get_activation(config["activation"]))
            
            if config["dropout"] > 0:
                fc_layers.append(nn.Dropout(config["dropout"]))
            
            fc_input = fc_output
        
        fc_layers.append(nn.Linear(fc_input, num_classes))
        self.fc_layers = nn.Sequential(*fc_layers)
    
    def _get_activation(self, name: str):
        activations = {
            "relu": nn.ReLU(),
            "leaky_relu": nn.LeakyReLU(),
            "elu": nn.ELU(),
            "selu": nn.SELU(),
            "gelu": nn.GELU()
        }
        return activations[name]
    
    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x

# ============================================================================
# TRAINING COMPONENTS
# ============================================================================

class ModelTrainer:
    """Handles model training and evaluation"""
    
    def __init__(self, train_loader, val_loader, device):
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
    
    def train_epoch(self, model, optimizer, criterion):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for inputs, targets in self.train_loader:
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        
        return total_loss / len(self.train_loader), 100. * correct / total
    
    def evaluate(self, model, criterion):
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in self.val_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        return total_loss / len(self.val_loader), 100. * correct / total
    
    def train_model(self, model, config, n_epochs=10, early_stopping_patience=3):
        criterion = nn.CrossEntropyLoss()
        optimizer = self._get_optimizer(model, config)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="max", factor=0.5, patience=2
        )
        
        best_val_acc = 0
        patience_counter = 0
        
        for epoch in range(n_epochs):
            train_loss, train_acc = self.train_epoch(model, optimizer, criterion)
            val_loss, val_acc = self.evaluate(model, criterion)
            
            scheduler.step(val_acc)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= early_stopping_patience:
                break
        
        return best_val_acc
    
    def _get_optimizer(self, model, config):
        if config["optimizer"] == "adam":
            return optim.Adam(model.parameters(), lr=config["learning_rate"])
        elif config["optimizer"] == "sgd":
            return optim.SGD(model.parameters(), lr=config["learning_rate"], momentum=0.9)
        elif config["optimizer"] == "adamw":
            return optim.AdamW(model.parameters(), lr=config["learning_rate"])

# ============================================================================
# NAS ENGINE
# ============================================================================

class NeuralArchitectureSearch:
    """Neural Architecture Search system"""
    
    def __init__(self, train_loader, val_loader, input_shape, num_classes, 
                 max_conv_layers=4, max_fc_layers=3):
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.max_conv_layers = max_conv_layers
        self.max_fc_layers = max_fc_layers
        self.device = device
        self.trainer = ModelTrainer(train_loader, val_loader, device)
        
        mlflow.set_experiment("neural_architecture_search")
    
    def define_search_space(self, trial: optuna.Trial) -> dict:
        config = {
            "num_conv_layers": trial.suggest_int("num_conv_layers", 1, self.max_conv_layers),
            "num_fc_layers": trial.suggest_int("num_fc_layers", 1, self.max_fc_layers),
            "activation": trial.suggest_categorical("activation", 
                ["relu", "leaky_relu", "elu", "gelu"]),
            "dropout": trial.suggest_float("dropout", 0.0, 0.5),
            "learning_rate": trial.suggest_loguniform("learning_rate", 1e-4, 1e-2),
            "optimizer": trial.suggest_categorical("optimizer", ["adam", "adamw", "sgd"]),
        }
        
        for i in range(config["num_conv_layers"]):
            config[f"conv_{i}_filters"] = trial.suggest_categorical(
                f"conv_{i}_filters", [32, 64, 128, 256]
            )
            config[f"conv_{i}_kernel"] = trial.suggest_categorical(
                f"conv_{i}_kernel", [3, 5]
            )
            config[f"conv_{i}_pooling"] = trial.suggest_categorical(
                f"conv_{i}_pooling", [True, False]
            )
        
        for i in range(config["num_fc_layers"]):
            config[f"fc_{i}_units"] = trial.suggest_categorical(
                f"fc_{i}_units", [64, 128, 256, 512]
            )
        
        return config
    
    def objective(self, trial: optuna.Trial) -> float:
        config = self.define_search_space(trial)
        
        with mlflow.start_run(nested=True):
            mlflow.log_params(config)
            
            try:
                model = NASNetwork(config, self.input_shape, self.num_classes).to(self.device)
                n_params = sum(p.numel() for p in model.parameters())
                mlflow.log_metric("n_parameters", n_params)
                
                val_accuracy = self.trainer.train_model(model, config, n_epochs=8)
                
                mlflow.log_metric("val_accuracy", val_accuracy)
                
                # Update session state
                st.session_state.current_trial = trial.number + 1
                st.session_state.trial_results.append({
                    "trial": trial.number,
                    "accuracy": val_accuracy,
                    "params": n_params,
                    "config": config
                })
                
                return val_accuracy
                
            except Exception as e:
                st.error(f"Trial {trial.number} failed: {str(e)}")
                return 0.0
    
    def search(self, n_trials=20, timeout=3600):
        st.session_state.total_trials = n_trials
        
        study = optuna.create_study(
            direction="maximize",
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=5)
        )
        
        study.optimize(
            self.objective,
            n_trials=n_trials,
            timeout=timeout,
        )
        
        return study

# ============================================================================
# DATA LOADING
# ============================================================================

@st.cache_resource
def load_data(train_samples=5000, val_samples=1000):
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root="./data", train=True, download=True, transform=transform_train
    )
    
    val_dataset = torchvision.datasets.CIFAR10(
        root="./data", train=False, download=True, transform=transform_test
    )
    
    train_subset = Subset(train_dataset, range(train_samples))
    val_subset = Subset(val_dataset, range(val_samples))
    
    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)
    
    return train_loader, val_loader

# ============================================================================
# STREAMLIT UI
# ============================================================================

def main():
    st.title("üß† Neural Architecture Search System")
    st.markdown("### Automated Neural Network Design with MLflow Tracking")
    
    # Sidebar configuration
    st.sidebar.header("‚öôÔ∏è Configuration")
    
    # Device info
    st.sidebar.info(f"**Device:** {device}")
    if torch.cuda.is_available():
        st.sidebar.success(f"**GPU:** {torch.cuda.get_device_name(0)}")
    
    # Search parameters
    st.sidebar.subheader("Search Parameters")
    n_trials = st.sidebar.slider("Number of Trials", 5, 100, 20)
    train_samples = st.sidebar.slider("Training Samples", 1000, 50000, 5000, step=1000)
    val_samples = st.sidebar.slider("Validation Samples", 500, 10000, 1000, step=500)
    max_conv_layers = st.sidebar.slider("Max Conv Layers", 1, 5, 3)
    max_fc_layers = st.sidebar.slider("Max FC Layers", 1, 4, 2)
    
    # Main content
    tab1, tab2, tab3, tab4 = st.tabs(["üöÄ Search", "üìä Results", "üèÜ Best Model", "üìà Analytics"])
    
    with tab1:
        st.header("Start Architecture Search")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Trials", n_trials)
        with col2:
            st.metric("Training Samples", train_samples)
        with col3:
            st.metric("Validation Samples", val_samples)
        
        if not st.session_state.search_running and not st.session_state.search_complete:
            if st.button("üöÄ Start Search", type="primary", use_container_width=True):
                st.session_state.search_running = True
                st.session_state.search_complete = False
                st.session_state.current_trial = 0
                st.session_state.trial_results = []
                
                with st.spinner("Loading dataset..."):
                    train_loader, val_loader = load_data(train_samples, val_samples)
                
                st.success("‚úì Dataset loaded!")
                
                # Initialize NAS
                nas = NeuralArchitectureSearch(
                    train_loader=train_loader,
                    val_loader=val_loader,
                    input_shape=(3, 32, 32),
                    num_classes=10,
                    max_conv_layers=max_conv_layers,
                    max_fc_layers=max_fc_layers
                )
                
                # Progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # Run search
                with st.spinner("Running Neural Architecture Search..."):
                    study = nas.search(n_trials=n_trials)
                    st.session_state.study = study
                    st.session_state.search_complete = True
                    st.session_state.search_running = False
                
                progress_bar.progress(100)
                status_text.success(f"‚úì Search complete! Best accuracy: {study.best_value:.2f}%")
                st.balloons()
                
        elif st.session_state.search_running:
            st.info("üîÑ Search in progress...")
            progress = st.session_state.current_trial / st.session_state.total_trials
            st.progress(progress)
            st.write(f"Trial {st.session_state.current_trial} / {st.session_state.total_trials}")
            
        elif st.session_state.search_complete:
            st.success("‚úÖ Search completed! Check other tabs for results.")
            if st.button("üîÑ Start New Search", use_container_width=True):
                st.session_state.search_complete = False
                st.session_state.study = None
                st.rerun()
    
    with tab2:
        st.header("Search Results")
        
        if st.session_state.study is not None:
            study = st.session_state.study
            
            # Summary metrics
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Best Accuracy", f"{study.best_value:.2f}%")
            with col2:
                st.metric("Total Trials", len(study.trials))
            with col3:
                completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])
                st.metric("Completed", completed)
            with col4:
                pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])
                st.metric("Pruned", pruned)
            
            # Optimization history
            st.subheader("Optimization History")
            fig = plot_optimization_history(study)
            fig.update_layout(height=500)
            st.plotly_chart(fig, use_container_width=True)
            
            # Parameter importance
            st.subheader("Hyperparameter Importance")
            try:
                fig2 = plot_param_importances(study)
                fig2.update_layout(height=500)
                st.plotly_chart(fig2, use_container_width=True)
            except:
                st.info("Not enough data for parameter importance analysis")
            
            # Trial results table
            st.subheader("Trial Results")
            if st.session_state.trial_results:
                import pandas as pd
                df = pd.DataFrame([
                    {
                        "Trial": r["trial"],
                        "Accuracy": f"{r[\'accuracy\']:.2f}%",
                        "Parameters": f"{r[\'params\']:,}",
                        "Conv Layers": r["config"]["num_conv_layers"],
                        "FC Layers": r["config"]["num_fc_layers"],
                        "Optimizer": r["config"]["optimizer"],
                    }
                    for r in st.session_state.trial_results
                ])
                st.dataframe(df, use_container_width=True)
        else:
            st.info("üëà Start a search to see results here")
    
    with tab3:
        st.header("Best Model Architecture")
        
        if st.session_state.study is not None:
            study = st.session_state.study
            
            st.success(f"üèÜ Best Validation Accuracy: {study.best_value:.2f}%")
            
            # Best hyperparameters
            st.subheader("Best Hyperparameters")
            col1, col2 = st.columns(2)
            
            with col1:
                st.json({k: v for k, v in list(study.best_params.items())[:len(study.best_params)//2]})
            with col2:
                st.json({k: v for k, v in list(study.best_params.items())[len(study.best_params)//2:]})
            
            # Build best model
            st.subheader("Model Architecture")
            config = study.best_params.copy()
            best_model = NASNetwork(config, (3, 32, 32), 10)
            
            st.code(str(best_model), language="python")
            
            n_params = sum(p.numel() for p in best_model.parameters())
            st.metric("Total Parameters", f"{n_params:,}")
            
            # Download config
            if st.button("üíæ Download Best Configuration", use_container_width=True):
                config_json = json.dumps(config, indent=2)
                st.download_button(
                    label="Download JSON",
                    data=config_json,
                    file_name="best_architecture.json",
                    mime="application/json"
                )
        else:
            st.info("üëà Start a search to see the best model here")
    
    with tab4:
        st.header("Performance Analytics")
        
        if st.session_state.trial_results:
            results = st.session_state.trial_results
            
            # Accuracy over trials
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=[r["trial"] for r in results],
                y=[r["accuracy"] for r in results],
                mode="lines+markers",
                name="Accuracy",
                line=dict(color="royalblue", width=2),
                marker=dict(size=8)
            ))
            fig.update_layout(
                title="Accuracy Evolution",
                xaxis_title="Trial",
                yaxis_title="Validation Accuracy (%)",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Parameters vs Accuracy
            fig2 = go.Figure()
            fig2.add_trace(go.Scatter(
                x=[r["params"] for r in results],
                y=[r["accuracy"] for r in results],
                mode="markers",
                marker=dict(
                    size=10,
                    color=[r["accuracy"] for r in results],
                    colorscale="Viridis",
                    showscale=True,
                    colorbar=dict(title="Accuracy")
                ),
                text=[f"Trial {r[\'trial\']}" for r in results],
                hovertemplate="<b>%{text}</b><br>Params: %{x:,}<br>Accuracy: %{y:.2f}%<extra></extra>"
            ))
            fig2.update_layout(
                title="Model Size vs Accuracy",
                xaxis_title="Number of Parameters",
                yaxis_title="Validation Accuracy (%)",
                height=400
            )
            st.plotly_chart(fig2, use_container_width=True)
            
            # Top 5 architectures
            st.subheader("Top 5 Architectures")
            sorted_results = sorted(results, key=lambda x: x["accuracy"], reverse=True)[:5]
            
            for i, r in enumerate(sorted_results, 1):
                with st.expander(f"Rank {i}: Trial {r[\'trial\']} - {r[\'accuracy\']:.2f}%"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**Architecture:**")
                        st.write(f"- Conv Layers: {r[\'config\'][\'num_conv_layers\']}")
                        st.write(f"- FC Layers: {r[\'config\'][\'num_fc_layers\']}")
                        st.write(f"- Activation: {r[\'config\'][\'activation\']}")
                        st.write(f"- Dropout: {r[\'config\'][\'dropout\']:.3f}")
                    with col2:
                        st.write("**Training:**")
                        st.write(f"- Optimizer: {r[\'config\'][\'optimizer\']}")
                        st.write(f"- Learning Rate: {r[\'config\'][\'learning_rate\']:.6f}")
                        st.write(f"- Parameters: {r[\'params\']:,}")
        else:
            st.info("üëà Start a search to see analytics here")
    
    # Footer
    st.markdown("---")
    st.markdown("Built with PyTorch, Optuna, MLflow & Streamlit | üß† Neural Architecture Search")

if __name__ == "__main__":
    main()
'''

# Write the Streamlit app to file
with open('nas_app.py', 'w') as f:
    f.write(streamlit_app)

print("‚úì Streamlit app created: nas_app.py")

# ============================================================================
# NGROK SETUP AND LAUNCH
# ============================================================================

from pyngrok import ngrok, conf
import subprocess
import time
import os

def setup_and_launch():
    print("\n" + "="*80)
    print("NEURAL ARCHITECTURE SEARCH - STREAMLIT INTERFACE")
    print("="*80)
    
    # Get ngrok token
    print("\nüîë Ngrok Authentication Required")
    print("Get your free token at: https://dashboard.ngrok.com/get-started/your-authtoken")
    ngrok_token = input("\nEnter your Ngrok authtoken: ").strip()
    
    if not ngrok_token:
        print("‚ùå No token provided. Exiting...")
        return
    
    # Set ngrok authtoken
    try:
        conf.get_default().auth_token = ngrok_token
        print("‚úì Ngrok token configured")
    except Exception as e:
        print(f"‚ùå Error configuring ngrok: {e}")
        return
    
    # Kill any existing Streamlit processes
    try:
        os.system("pkill -f streamlit")
        time.sleep(2)
    except:
        pass
    
    print("\nüöÄ Starting Streamlit server...")
    
    # Start Streamlit in background
    process = subprocess.Popen(
        ["streamlit", "run", "nas_app.py", "--server.port", "8501", "--server.headless", "true"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # Wait for Streamlit to start
    time.sleep(10)
    
    try:
        # Create ngrok tunnel
        print("üåê Creating ngrok tunnel...")
        public_url = ngrok.connect(8501, bind_tls=True)
        
        print("\n" + "="*80)
        print("‚úÖ NEURAL ARCHITECTURE SEARCH IS READY!")
        print("="*80)
        print(f"\nüîó Access your NAS interface at:")
        print(f"\n   {public_url}")
        print("\n" + "="*80)
        print("\nüìù Instructions:")
        print("   1. Click the URL above to open the interface")
        print("   2. Configure search parameters in the sidebar")
        print("   3. Click 'Start Search' to begin")
        print("   4. Monitor progress in real-time")
        print("   5. View results, best model, and analytics")
        print("\nüí° Tips:")
        print("   - Start with 10-20 trials for quick testing")
        print("   - Use GPU for faster training (automatic if available)")
        print("   - All experiments are tracked with MLflow")
        print("   - Download best architecture config from Best Model tab")
        print("\n‚ö†Ô∏è  Keep this cell running to maintain the connection")
        print("="*80 + "\n")
        
        # Keep running
        try:
            process.wait()
        except KeyboardInterrupt:
            print("\n\nüõë Shutting down...")
            ngrok.disconnect(public_url)
            process.terminate()
            print("‚úì Cleanup complete")
            
    except Exception as e:
        print(f"\n‚ùå Error creating tunnel: {e}")
        print("\nTroubleshooting:")
        print("1. Verify your ngrok token is correct")
        print("2. Check if you have active tunnels at: https://dashboard.ngrok.com/")
        print("3. Free ngrok accounts are limited to 1 tunnel at a time")
        process.terminate()

# Launch the application
if __name__ == "__main__":
    setup_and_launch()
