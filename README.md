# Neural Architecture Search (NAS) System

---

## üìö Table of Contents

1. [Overview](#overview)
2. [Features](#features)
3. [Installation & Setup](#installation--setup)
4. [Quick Start Guide](#quick-start-guide)
5. [System Architecture](#system-architecture)
6. [Configuration Options](#configuration-options)
7. [Understanding the Results](#understanding-the-results)
8. [Advanced Usage](#advanced-usage)
9. [Troubleshooting](#troubleshooting)
10. [API Reference](#api-reference)
11. [Best Practices](#best-practices)

---

## üéØ Overview

The Neural Architecture Search (NAS) System is an automated machine learning tool that designs optimal neural network architectures without manual intervention. It uses Bayesian optimization (via Optuna) to explore the architecture space and find the best-performing models for your task.

### Key Capabilities

- **Automated Architecture Design**: Discovers optimal network structures automatically
- **Hyperparameter Optimization**: Tunes learning rate, optimizer, dropout, and more
- **Real-time Monitoring**: Track search progress through interactive web interface
- **Experiment Tracking**: Complete MLflow integration for reproducibility
- **GPU Acceleration**: Automatic detection and utilization of CUDA devices

### Technology Stack

- **PyTorch**: Deep learning framework
- **Optuna**: Hyperparameter optimization
- **MLflow**: Experiment tracking and model versioning
- **Streamlit**: Interactive web interface
- **Ngrok**: Secure tunneling for Colab access

---

## üåü Features

### 1. Dynamic Network Construction
- Variable number of convolutional layers (1-5)
- Variable number of fully connected layers (1-4)
- Multiple activation functions (ReLU, LeakyReLU, ELU, GELU)
- Configurable filter sizes, kernel sizes, and pooling strategies

### 2. Search Space Parameters

| Parameter | Options | Description |
|-----------|---------|-------------|
| Conv Layers | 1-5 | Number of convolutional layers |
| FC Layers | 1-4 | Number of fully connected layers |
| Filters | 32, 64, 128, 256 | Number of filters per conv layer |
| Kernel Size | 3√ó3, 5√ó5 | Convolutional kernel dimensions |
| Activation | ReLU, LeakyReLU, ELU, GELU | Activation function type |
| Dropout | 0.0 - 0.5 | Dropout rate |
| Learning Rate | 1e-4 - 1e-2 | Optimizer learning rate |
| Optimizer | Adam, AdamW, SGD | Optimization algorithm |
| Pooling | True/False | Max pooling after conv layers |

### 3. Interactive Web Interface

#### üöÄ Search Tab
- Configure search parameters
- Start/stop architecture search
- Real-time progress tracking
- Dataset loading status

#### üìä Results Tab
- Optimization history visualization
- Hyperparameter importance analysis
- Trial results table
- Success/failure statistics

#### üèÜ Best Model Tab
- Best architecture visualization
- Complete hyperparameter configuration
- Model size and parameter count
- Downloadable JSON config

#### üìà Analytics Tab
- Accuracy evolution over trials
- Model size vs. accuracy scatter plot
- Top 5 architectures comparison
- Detailed configuration breakdown

---

## üöÄ Installation & Setup

### Prerequisites
- Google Colab account (or local Jupyter)
- Ngrok account (free tier sufficient)
- Python 3.7+
- CUDA-capable GPU (optional but recommended)

### Step 1: Get Ngrok Token

1. Visit [https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)
2. Sign up for a free account
3. Copy your authtoken

### Step 2: Run in Google Colab

```python
# 1. Copy the entire NAS system code into a Colab cell
# 2. Run the cell
# 3. When prompted, paste your Ngrok token
# 4. Wait for the public URL to appear
# 5. Click the URL to open the interface
```

### Step 3: Local Installation (Optional)

```bash
# Install dependencies
pip install torch torchvision optuna mlflow streamlit pyngrok plotly

# Run the script
python nas_system.py

# Enter your Ngrok token when prompted
```

---

## üìñ Quick Start Guide

### Basic Workflow

1. **Access Interface**
   - Run the code in Colab
   - Enter Ngrok token
   - Click the generated URL

2. **Configure Search**
   - Navigate to sidebar
   - Set number of trials (start with 10-20)
   - Adjust training/validation samples
   - Configure max layers

3. **Start Search**
   - Go to "üöÄ Search" tab
   - Click "Start Search" button
   - Monitor progress bar

4. **View Results**
   - Check "üìä Results" tab for optimization history
   - Explore "üèÜ Best Model" for optimal architecture
   - Analyze "üìà Analytics" for detailed insights

5. **Download Best Model**
   - Navigate to "üèÜ Best Model" tab
   - Click "Download Best Configuration"
   - Save JSON file for future use

---

## üèóÔ∏è System Architecture

### Component Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Streamlit Web Interface              ‚îÇ
‚îÇ  (User Interaction & Visualization Layer)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Neural Architecture Search Engine         ‚îÇ
‚îÇ         (Optuna Optimization)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Model Trainer ‚îÇ  ‚îÇ   MLflow    ‚îÇ
‚îÇ   (PyTorch)    ‚îÇ  ‚îÇ  Tracking   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ NASNetwork     ‚îÇ
‚îÇ (Dynamic NN)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

#### 1. NASNetwork Class
Dynamically constructs neural networks based on configuration dictionary.

**Features:**
- Variable architecture depth
- Modular layer construction
- Automatic dimension calculation
- Flexible activation functions

#### 2. ModelTrainer Class
Handles training, validation, and optimization.

**Features:**
- Training loop management
- Early stopping mechanism
- Learning rate scheduling
- Performance evaluation

#### 3. NeuralArchitectureSearch Class
Main NAS engine coordinating the search process.

**Features:**
- Search space definition
- Optuna trial management
- MLflow integration
- Result tracking

#### 4. Streamlit Interface
Web-based UI for interaction and visualization.

**Features:**
- Real-time progress updates
- Interactive plots
- Configuration controls
- Result export

---

## ‚öôÔ∏è Configuration Options

### Search Parameters

#### Number of Trials
```python
n_trials = 20  # Default: 20
# Range: 5-100
# Recommendation: 
#   - Quick test: 5-10
#   - Standard: 20-50
#   - Thorough: 50-100
```

#### Training Samples
```python
train_samples = 5000  # Default: 5000
# Range: 1000-50000
# Recommendation:
#   - Fast iteration: 1000-3000
#   - Balanced: 5000-10000
#   - High accuracy: 20000-50000
```

#### Validation Samples
```python
val_samples = 1000  # Default: 1000
# Range: 500-10000
# Recommendation:
#   - Minimum: 500
#   - Standard: 1000-2000
#   - Robust: 5000-10000
```

#### Max Convolutional Layers
```python
max_conv_layers = 3  # Default: 3
# Range: 1-5
# Impact:
#   - Fewer layers: Faster, simpler models
#   - More layers: Higher capacity, slower training
```

#### Max Fully Connected Layers
```python
max_fc_layers = 2  # Default: 2
# Range: 1-4
# Impact:
#   - Affects model expressiveness
#   - More layers = more parameters
```

---

## üìä Understanding the Results

### Metrics Explained

#### 1. Validation Accuracy
- **Definition**: Percentage of correctly classified samples on validation set
- **Range**: 0-100%
- **Good Performance**: >70% on CIFAR-10
- **Excellent Performance**: >85% on CIFAR-10

#### 2. Number of Parameters
- **Definition**: Total trainable weights in the model
- **Typical Range**: 10K - 10M parameters
- **Considerations**:
  - More parameters = higher capacity
  - Too many = overfitting risk
  - Fewer = faster inference

#### 3. Trial State
- **COMPLETE**: Trial finished successfully
- **PRUNED**: Trial stopped early (poor performance)
- **FAILED**: Trial encountered error
- **RUNNING**: Trial currently executing

### Visualization Interpretations

#### Optimization History Plot
```
Shows validation accuracy over trials
- Upward trend = search is improving
- Plateau = convergence reached
- Fluctuations = exploration of space
```

#### Hyperparameter Importance Plot
```
Ranks parameters by impact on accuracy
- High importance = critical parameter
- Low importance = less sensitive
- Guides future search focus
```

#### Accuracy Evolution Plot
```
Line plot of trial-by-trial performance
- Early trials = exploration
- Later trials = exploitation
- Best line = maximum achieved
```

#### Model Size vs Accuracy Scatter
```
Reveals efficiency-accuracy tradeoffs
- Top-left = efficient models
- Top-right = powerful models
- Bottom = poor performers
```

---

## üéì Advanced Usage

### Custom Search Space

Modify the `define_search_space` method:

```python
def define_search_space(self, trial):
    config = {
        # Add custom parameters
        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),
        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-5, 1e-3),
        
        # Modify existing ranges
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),
        
        # Add conditional parameters
        'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),
    }
    return config
```

### Custom Dataset

Replace CIFAR-10 with your own data:

```python
def load_custom_data():
    # Load your dataset
    train_dataset = YourCustomDataset(train=True)
    val_dataset = YourCustomDataset(train=False)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    
    return train_loader, val_loader
```

### Extended Training

Increase epochs for better performance:

```python
# In ModelTrainer.train_model
val_accuracy = self.trainer.train_model(
    model, 
    config, 
    n_epochs=20,  # Increased from 8
    early_stopping_patience=5  # Increased patience
)
```

### Multi-GPU Support

Enable data parallelism:

```python
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
model = model.to(device)
```

---

## üîß Troubleshooting

### Common Issues

#### 1. "Trial failed: CUDA out of memory"

**Solution:**
- Reduce batch size
- Reduce max_conv_layers
- Use fewer training samples
- Clear GPU cache: `torch.cuda.empty_cache()`

#### 2. "Ngrok tunnel error"

**Causes:**
- Invalid authtoken
- Multiple tunnels on free plan
- Network connectivity issues

**Solutions:**
- Verify token at ngrok.com
- Close other ngrok tunnels
- Check internet connection
- Try regenerating token

#### 3. "Module not found"

**Solution:**
```python
# Reinstall dependencies
!pip install --upgrade torch torchvision optuna mlflow streamlit pyngrok
```

#### 4. Slow performance

**Solutions:**
- Enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU
- Reduce number of trials
- Use fewer training samples
- Reduce max layers

#### 5. All trials failing

**Causes:**
- Invalid search space configuration
- Insufficient memory
- Data loading issues

**Solutions:**
- Check error messages in trial results
- Simplify search space
- Reduce model complexity
- Verify dataset availability

---

## üìö API Reference

### NASNetwork

```python
class NASNetwork(nn.Module):
    """
    Dynamically constructed neural network.
    
    Parameters:
    -----------
    config : dict
        Architecture configuration from Optuna trial
    input_shape : tuple
        Input tensor shape (C, H, W)
    num_classes : int
        Number of output classes
    
    Methods:
    --------
    forward(x) : torch.Tensor
        Forward pass through the network
    """
```

### ModelTrainer

```python
class ModelTrainer:
    """
    Handles model training and evaluation.
    
    Parameters:
    -----------
    train_loader : DataLoader
        Training data loader
    val_loader : DataLoader
        Validation data loader
    device : torch.device
        Computing device (CPU/GPU)
    
    Methods:
    --------
    train_epoch(model, optimizer, criterion) : tuple
        Train for one epoch, returns (loss, accuracy)
        
    evaluate(model, criterion) : tuple
        Evaluate on validation set, returns (loss, accuracy)
        
    train_model(model, config, n_epochs, early_stopping_patience) : float
        Complete training pipeline, returns best validation accuracy
    """
```

### NeuralArchitectureSearch

```python
class NeuralArchitectureSearch:
    """
    Main NAS engine.
    
    Parameters:
    -----------
    train_loader : DataLoader
        Training data loader
    val_loader : DataLoader
        Validation data loader
    input_shape : tuple
        Input tensor shape
    num_classes : int
        Number of classes
    max_conv_layers : int
        Maximum convolutional layers to explore
    max_fc_layers : int
        Maximum fully connected layers to explore
    
    Methods:
    --------
    define_search_space(trial) : dict
        Define architecture search space
        
    objective(trial) : float
        Optuna objective function
        
    search(n_trials, timeout) : optuna.Study
        Run neural architecture search
    """
```

---

## üí° Best Practices

### 1. Start Small, Scale Up

```python
# Phase 1: Quick exploration (30 min)
n_trials = 10
train_samples = 2000
val_samples = 500

# Phase 2: Refined search (2 hours)
n_trials = 30
train_samples = 5000
val_samples = 1000

# Phase 3: Final optimization (4+ hours)
n_trials = 50
train_samples = 20000
val_samples = 5000
```

### 2. Monitor Resource Usage

```python
# Check GPU utilization
!nvidia-smi

# Monitor memory
import torch
print(f"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
```

### 3. Experiment Naming

```python
# Use descriptive MLflow experiment names
mlflow.set_experiment(f"nas_{dataset}_{datetime.now().strftime('%Y%m%d_%H%M')}")
```

### 4. Save Intermediate Results

```python
# Periodically export study
import pickle

with open('study_checkpoint.pkl', 'wb') as f:
    pickle.dump(study, f)
```

### 5. Hyperparameter Ranges

- **Learning Rate**: Use log-uniform distribution (1e-4 to 1e-2)
- **Dropout**: Linear distribution (0.0 to 0.5)
- **Layers**: Integer distribution based on task complexity
- **Filters**: Categorical powers of 2 (32, 64, 128, 256)

### 6. Evaluation Strategy

```python
# Use stratified sampling for imbalanced datasets
from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2)
train_idx, val_idx = next(splitter.split(X, y))
```

### 7. Result Interpretation

- **High variance across trials**: Increase training epochs
- **All similar accuracy**: Search space too narrow
- **Many failed trials**: Reduce model complexity
- **Plateau after few trials**: Search space too simple

---

## üìà Performance Benchmarks

### Expected Results (CIFAR-10)

| Configuration | Trials | Time | Best Accuracy | Avg Accuracy |
|--------------|--------|------|---------------|--------------|
| Quick Test | 10 | 30 min | 65-75% | 60-70% |
| Standard | 20 | 1 hour | 70-80% | 65-75% |
| Thorough | 50 | 3 hours | 75-85% | 70-80% |
| Extensive | 100 | 6+ hours | 80-90% | 75-85% |

*Benchmarks using Tesla T4 GPU, 5000 training samples, 8 epochs per trial*

### Resource Requirements

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|---------|
| RAM | 4 GB | 8 GB | 16 GB |
| GPU Memory | 4 GB | 8 GB | 16 GB |
| Storage | 1 GB | 2 GB | 5 GB |
| CPU Cores | 2 | 4 | 8 |

---

## üéØ Use Cases

### 1. Image Classification
- CIFAR-10/100
- ImageNet subset
- Medical imaging
- Satellite imagery

### 2. Research Applications
- Architecture search studies
- Meta-learning experiments
- AutoML benchmarking
- Neural architecture analysis

### 3. Production Systems
- Model compression
- Edge device optimization
- Resource-constrained deployment
- Automated model updates

### 4. Educational Projects
- Deep learning demonstrations
- AutoML tutorials
- Optimization workshops
- ML course projects

---

## üîó Additional Resources

### Documentation
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [MLflow Documentation](https://mlflow.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)

### Research Papers
- Neural Architecture Search: A Survey (2019)
- Efficient Neural Architecture Search (ENAS)
- DARTS: Differentiable Architecture Search
- Auto-Keras: Efficient Neural Architecture Search

### Community
- [Optuna GitHub](https://github.com/optuna/optuna)
- [PyTorch Forums](https://discuss.pytorch.org/)
- [MLflow Slack](https://mlflow.org/community)

---

## üìù License & Citation

This project is provided as-is for educational and research purposes.

### Citation

```bibtex
@software{nas_system_2024,
  title={Neural Architecture Search System with Streamlit Interface},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/nas-system}
}
```

---

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

1. **New Search Strategies**: Implement evolutionary algorithms, reinforcement learning
2. **Additional Datasets**: Support for more datasets beyond CIFAR-10
3. **Model Export**: ONNX export, TensorFlow conversion
4. **Visualization**: More interactive plots, architecture diagrams
5. **Optimization**: Distributed training, mixed precision




---

## üéì Conclusion

The Neural Architecture Search system provides a complete, production-ready solution for automated neural network design. With its intuitive interface, powerful optimization engine, and comprehensive tracking, it enables both researchers and practitioners to discover optimal architectures efficiently.

**Key Takeaways:**
- Start with small trials to understand the system
- Use GPU acceleration for best performance
- Monitor and analyze results continuously
- Export and document best architectures
- Iterate and refine based on insights

Happy searching! üöÄ

---

*Last Updated: 2024*
*Version: 1.0.0*
